{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RCLC Baseline\n",
    "\n",
    "## General Description of the model\n",
    "Our model is composed of the modules:\n",
    "* Question-Answering (QA) system (DocumentQA). Provides the answers\n",
    "* Entity typing of the answers (Ultra Fine Entity Typing)\n",
    "* Rescoring of the answers. A neural network that takes the score of the QA system and the probability of an answer to be a of a certain type (10k types). The output is the final confidence score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the input data\n",
    "You can skip to Execution of the Model if you have already prepared the input data\n",
    "\n",
    "### Download the data\n",
    "Download the corpus from https://github.com/Coleridge-Initiative/rclc\n",
    "\n",
    "### Convert the data into .txt files\n",
    "Follow the steps in https://github.com/LARC-CMU-SMU/rclc_2019_baseline to convert the corpus into .txt files (kudos to @philipskokoh)\n",
    "\n",
    "\n",
    "Put the txt files in ../data/input/files/text/\n",
    "\n",
    "The input data we have is composed by a .jsonld file with the metadata information about publications and datasets and a folder with all the publications in .txt format.\n",
    "\n",
    "Our model assumes the existence of a .json file with the metadata information about the publications so first, we will create this file. This file has the following format:\n",
    "\n",
    "```\n",
    "{'publication_id': 0,\n",
    " 'unique_identifier': '9bab608b09ad5834ecf9',\n",
    " 'title': '9bab608b09ad5834ecf9',\n",
    " 'pub_date': '2015-10-15',\n",
    " 'pdf_file_name': '9bab608b09ad5834ecf9.pdf',\n",
    " 'text_file_name': '9bab608b09ad5834ecf9.pdf.txt'}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../data/input/files/text/\"\n",
    "list_txt = os.listdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['9bab608b09ad5834ecf9.pdf.txt', 'bbc5405690917684db8c.pdf.txt']"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_txt[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The publication with id 2b4497873374d080d359 should not be included because there is a problem with the conversion into txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-163-0d41b12db166>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m\"2b4497873374d080d359.pdf.txt\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist_txt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#Remove it from the folder and from the list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "assert not \"2b4497873374d080d359.pdf.txt\" in list_txt\n",
    "#Remove it from the folder and from the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_txt.remove(\"2b4497873374d080d359.pdf.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_num_id2original_id = {} \n",
    "#Our model assumes that the ids are numbers, but the ids of the datasets are strings, so we will\n",
    "# create this dictionary to transform from num_id to string_id (the original one)\n",
    "list_json_datasets = []\n",
    "for i, file in enumerate(list_txt):\n",
    "    pub_id = file[0:-8]\n",
    "    list_json_datasets.append({\n",
    "                    \"publication_id\": i ,\n",
    "                    \"unique_identifier\": pub_id,\n",
    "                    \"title\": pub_id,\n",
    "                    \"pub_date\": \"2015-10-15\",\n",
    "                    \"pdf_file_name\": file[0:-4],\n",
    "                    \"text_file_name\": file\n",
    "                    })\n",
    "    dict_num_id2original_id[i] = pub_id\n",
    "    os.rename(path + file, path + str(i) + \".txt\") \n",
    "    #Rename the file because the model assumes that the name of the files are the same as their ids (numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'publication_id': 2,\n",
       " 'unique_identifier': '74c81f06754918327e5d',\n",
       " 'title': '74c81f06754918327e5d',\n",
       " 'pub_date': '2015-10-15',\n",
       " 'pdf_file_name': '74c81f06754918327e5d.pdf',\n",
       " 'text_file_name': '74c81f06754918327e5d.pdf.txt'}"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_json_datasets[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "479"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list_json_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/input/publications.json', 'w+') as fout:\n",
    "    json.dump(list_json_datasets, fout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to save this dictionary because we need to convert from num_id to string_id (original id) during the evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./dict_num_id2original_id.json', 'w+') as f:\n",
    "    json.dump(dict_num_id2original_id, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution of the model\n",
    "Execute from the terminal the file code.sh . It executes the model and write the outputs in the folder data/output/data_set_mentions.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m nltk.downloader punkt stopwords\n",
    "%run ./docqa/scripts/run_on_user_documents_eval_final-mod_ph2_.py ./pretrain/cpu\n",
    "%run ./ner/DocQA2UltraFine.py\n",
    "%run ./ner/open_type/rcc_ultra_fine_main_v2.py release_model -lstm_type single -enhanced_mention -data_setup joint -add_crowd -multitask -mode test -reload_model_name release_model -load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from fuzzywuzzy import fuzz\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_model_path = '../data/output/data_set_mentions.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output from DocQA\n",
    "with open(output_model_path, 'r') as f:\n",
    "    output_model = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'publication_id': 0,\n",
       " 'mention': 'National Health and Nutrition Examination Survey Data',\n",
       " 'score': 0.38750842213630676}"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_model[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../corpus.jsonld\", 'r') as f:\n",
    "    corpus = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dict_num_id2original_id.json', 'r') as f:\n",
    "    dict_num_id2original_id = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_original_id2num_id = {v: k for k, v in dict_num_id2original_id.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a dictionary <Dataset id, dataset metadada> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_datasets = {}\n",
    "for instance in corpus[\"@graph\"]:\n",
    "    if instance[\"@type\"] == \"Dataset\":\n",
    "        dataset_id = instance[\"@id\"].split(\"-\")[-1]\n",
    "        dict_datasets[dataset_id] = instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'@id': 'https://github.com/Coleridge-Initiative/adrf-onto/wiki/Vocabulary#dataset-b5ecda6a60ca5c2476d2',\n",
       " '@type': 'Dataset',\n",
       " 'dct:alternative': [{'@value': 'Quarterly Food-at-Home Price Database'},\n",
       "  {'@value': 'QFAFHP'},\n",
       "  {'@value': 'QFAHPD'},\n",
       "  {'@value': 'FAH'},\n",
       "  {'@value': 'Quarterly Food at Home Prices Database'}],\n",
       " 'dct:publisher': {'@value': 'U.S. Department of Agriculture'},\n",
       " 'dct:title': {'@value': 'Quarterly Food at Home Prices'},\n",
       " 'foaf:page': {'@type': 'xsd:anyURI',\n",
       "  '@value': 'https://www.ers.usda.gov/data-products/quarterly-food-at-home-price-database'}}"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_datasets['b5ecda6a60ca5c2476d2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Given a dataset id, retrieve the list of names of this dataset\n",
    "'''\n",
    "def get_list_dataset_names(dataset_id):\n",
    "    dataset = dict_datasets[dataset_id]\n",
    "    list_names = [dataset['dct:title']['@value']]\n",
    "    if \"dct:alternative\" in dataset:\n",
    "        if type(dataset[\"dct:alternative\"]) is list:\n",
    "            for d in dataset[\"dct:alternative\"]:\n",
    "                list_names.append(d[\"@value\"])\n",
    "        else:\n",
    "            list_names.append(dataset[\"dct:alternative\"][\"@value\"])\n",
    "    return list_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a dictionary that given a dataset id, returns the list of aliases of that dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_list_names_datasets = {}\n",
    "for key in dict_datasets.keys():\n",
    "    dict_list_names_datasets[key] = get_list_dataset_names(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entity Linking Model\n",
    "From a string mention of the dataset to its dataset id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Retrieve the id of a dataset given its string mention in a publication.\n",
    "This is the Entity Linking model\n",
    "'''\n",
    "def get_id_dataset_name(dataset_name, dict_list_names_datasets):\n",
    "    dict_dist = {}\n",
    "    for key in dict_list_names_datasets.keys():\n",
    "        list_aliases = dict_list_names_datasets[key]\n",
    "        list_dist = []\n",
    "        for alias in list_aliases:\n",
    "            list_dist.append(fuzz.token_set_ratio(alias, dataset_name))\n",
    "        dict_dist[key] = max(list_dist)\n",
    "    return max(dict_dist.items(), key=lambda x: x[1])[0]\n",
    "assert get_id_dataset_name(\"NHANES\", dict_list_names_datasets) == '0a7b604ab2e52411d45a'\n",
    "assert get_id_dataset_name(\"NHANES data\", dict_list_names_datasets) == '0a7b604ab2e52411d45a'\n",
    "assert get_id_dataset_name(\"NHANES dataset\", dict_list_names_datasets) == '0a7b604ab2e52411d45a'\n",
    "assert get_id_dataset_name(\"ICDB\", dict_list_names_datasets) == '22571eb2d0cf42459c19'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select the best 5 answers per publication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_dict(d, k, v):\n",
    "    if k in d:\n",
    "        d[k].append(v)\n",
    "    else:\n",
    "        d[k] = [v]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_pub2answers = {}\n",
    "for instance in output_model:\n",
    "    pub_id = instance['publication_id'] \n",
    "    update_dict(dict_pub2answers, pub_id, (instance['mention'], instance['score']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('National Health and Nutrition Examination Survey Data', 0.38750842213630676), ('NHANES', 0.5256460905075073), ('NHANES', 0.6468685865402222), ('National Heath and Nutrition Examination Surveys', 0.5594545006752014)]\n"
     ]
    }
   ],
   "source": [
    "print(dict_pub2answers[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_y_preds = {}\n",
    "for key in dict_pub2answers.keys():\n",
    "    list_answers = sorted(dict_pub2answers[key], key=lambda t: t[1], reverse=True)[:5]\n",
    "    dict_y_preds[str(key)] = [pred for (pred, score) in list_answers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Investigation Approach',\n",
       " 'Empirical Implications',\n",
       " 'empirical implications',\n",
       " 'Corporate taxation data']"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_y_preds['1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f891c2809484c67a29c05bad92f0be5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=474), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dict_y_pred_ids = {}\n",
    "for key in tqdm_notebook(dict_y_preds.keys()):\n",
    "    list_preds_ids = []\n",
    "    for pred in dict_y_preds[key]:\n",
    "        pred_id = get_id_dataset_name(pred, dict_list_names_datasets)\n",
    "        list_preds_ids.append(pred_id)\n",
    "    list_preds_ids = list(OrderedDict.fromkeys(list_preds_ids))\n",
    "    while len(list_preds_ids) < 5:\n",
    "        list_preds_ids.append(\"\")\n",
    "    dict_y_pred_ids[key] = list_preds_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0a7b604ab2e52411d45a', '', '', '', '']"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_y_pred_ids['0']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve gold answers\n",
    "\n",
    "- <publication id (num), List<dataset names\\>>  #We may not need this\n",
    "- <publication id (num), List<dataset ids\\>> (dict_golden_ans_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_golden_ans = {}\n",
    "dict_golden_ans_ids = {}\n",
    "for instance in corpus[\"@graph\"]:\n",
    "    if instance[\"@type\"] == \"ResearchPublication\":\n",
    "        publication_id = instance[\"@id\"].split(\"-\")[-1]\n",
    "        if publication_id != \"2b4497873374d080d359\": # BLANK TXT, SO SKIP\n",
    "            list_dict_data_set_citation = []\n",
    "            num_id = dict_original_id2num_id[publication_id]\n",
    "\n",
    "            list_datasets = instance['cito:citesAsDataSource']\n",
    "            if not type(instance['cito:citesAsDataSource']) is list:\n",
    "                list_datasets = [instance['cito:citesAsDataSource']]\n",
    "        \n",
    "            list_datasets_ids = []\n",
    "            for dataset in list_datasets:\n",
    "                dataset_id = dataset[\"@id\"].split(\"-\")[-1]\n",
    "                list_datasets_ids.append(dataset_id)\n",
    "                list_dict_data_set_citation.append(\n",
    "                    get_list_dataset_names(dataset_id)\n",
    "                )\n",
    "            dict_golden_ans[num_id] = list_dict_data_set_citation\n",
    "            dict_golden_ans_ids[num_id] = list_datasets_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0a7b604ab2e52411d45a']"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_golden_ans_ids['0']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top5UpToD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "From https://github.com/LARC-CMU-SMU/rclc_2019_baseline/blob/master/src/eval_utils.py\n",
    "'''\n",
    "def top5UptoD_prec(y_true, y_pred):\n",
    "    \"\"\" Evaluation metric used by rclc.\n",
    "        This metric prioritizes precision and take into account a variable\n",
    "        number of datasets per publication.\n",
    "        Reference: https://github.com/Coleridge-Initiative/rclc#evaluation\n",
    "        Input:\n",
    "            - y_true: ground truths\n",
    "            - y_pred: top 5 predictions\n",
    "    \"\"\"\n",
    "    if len(y_true) == 0:\n",
    "        raise ValueError(f'Error: No ground truth!')\n",
    "    if len(y_pred) != 5:\n",
    "        raise ValueError(f'the length of y_pred must equal to 5.')\n",
    "\n",
    "    _D = len(y_true) if len(y_true) <= 5 else 5\n",
    "    correct = 0\n",
    "    error = 0\n",
    "    m = 0\n",
    "    for di in y_pred:\n",
    "        if di in y_true:\n",
    "            correct += 1\n",
    "        else:\n",
    "            error += 1\n",
    "        m += 1\n",
    "        if correct == _D:\n",
    "            break\n",
    "    return (correct * 1.) / m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.631906750174\n"
     ]
    }
   ],
   "source": [
    "list_scores = []\n",
    "for key in dict_golden_ans_ids.keys():\n",
    "    if key in dict_y_pred_ids:\n",
    "        score = top5UptoD_prec(dict_golden_ans_ids[key], dict_y_pred_ids[key])\n",
    "        list_scores.append(score)\n",
    "    else:\n",
    "        list_scores.append(0)\n",
    "print(np.mean(list_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rclc",
   "language": "python",
   "name": "rclc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
